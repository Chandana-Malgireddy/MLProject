{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chandana-Malgireddy/MLProject/blob/main/Final_Multimodal_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rrj1dUm3P7T"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload kaggle.json here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S7IOIS63VGJ"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nnmyUz43YaR"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/data\n",
        "!kaggle datasets download -d vikashrajluhaniwal/fashion-images -p /content/data\n",
        "!unzip -q \"/content/data/*.zip\" -d /content/data/fashion-images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXJ7H9KE4LoV"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle transformers datasets torch torchvision pillow faiss-cpu gradio pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPI0VH9yEVZK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import CLIPModel, CLIPProcessor, get_linear_schedule_with_warmup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJg9Sn8iEXos"
      },
      "outputs": [],
      "source": [
        "LR = 1e-5\n",
        "NUM_EPOCHS = 5\n",
        "WARMUP_RATIO = 0.05\n",
        "BATCH_SIZE = 64   # GPU-friendly\n",
        "BASE_DIR = \"/content/data/fashion-images/data\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wclUSndWEaOl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/data/fashion-images/data/fashion.csv\")\n",
        "# Build absolute paths\n",
        "df[\"image_path\"] = df.apply(\n",
        "    lambda row: os.path.join(\n",
        "        BASE_DIR,\n",
        "        row[\"Category\"],\n",
        "        row[\"Gender\"],\n",
        "        \"Images\",\n",
        "        \"images_with_product_ids\",\n",
        "        row[\"Image\"]\n",
        "    ),\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFCPqZSwEe3q"
      },
      "outputs": [],
      "source": [
        "df = df[df[\"image_path\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(\"Images available:\", len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzDqe3UKPBer"
      },
      "outputs": [],
      "source": [
        "df[\"caption\"] = (\n",
        "    \"Gender: \" + df[\"Gender\"] + \"; \"\n",
        "    \"Color: \" + df[\"Colour\"] + \"; \"\n",
        "    \"Category: \" + df[\"Category\"] + \"; \"\n",
        "    \"ProductType: \" + df[\"ProductType\"] + \"; \"\n",
        "    \"SubCategory: \" + df[\"SubCategory\"] + \"; \"\n",
        "    \"Usage: \" + df[\"Usage\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tv7YilQEhZJ"
      },
      "outputs": [],
      "source": [
        "class FashionDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        txt = row[\"caption\"]\n",
        "        return {\"image\": img, \"text\": txt}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-63R-2XQFPE3"
      },
      "outputs": [],
      "source": [
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "#For images:Resize/crop to CLIP’s expected size (e.g. 224×224),Normalize with CLIP’s mean/std,\n",
        "#Stack into a tensor pixel_values of shape (batch_size, 3, H, W).\n",
        "\n",
        "#For texts:Tokenize the strings,Add special tokens,\n",
        "#padding=True → pad all sequences in the batch to the same length,\n",
        "#truncation=True → cut off texts that are too long,\n",
        "def collate_fn(batch):\n",
        "    images = [b[\"image\"] for b in batch]\n",
        "    texts  = [b[\"text\"]  for b in batch]\n",
        "\n",
        "    encoding = processor(\n",
        "        images=images,\n",
        "        text=texts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fO3VkuPxKMv"
      },
      "outputs": [],
      "source": [
        "!pip install imagehash\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgZTdo7Vir7A"
      },
      "outputs": [],
      "source": [
        "import imagehash\n",
        "from PIL import Image\n",
        "def compute_phash(img_path):\n",
        "    try:\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        return str(imagehash.phash(img))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "df[\"phash\"] = df[\"image_path\"].apply(compute_phash)\n",
        "df = df.drop_duplicates(subset=[\"phash\"]).reset_index(drop=True)\n",
        "print(\"Unique images after pHash:\", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mErFWz1FSWo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# ======================================================\n",
        "# 80/20 TRAIN–TEST SPLIT\n",
        "# ======================================================\n",
        "train_df, test_df = train_test_split(df,test_size=0.2,random_state=42,shuffle=True)\n",
        "\n",
        "print(\"Train samples:\", len(train_df))\n",
        "print(\"Test samples:\", len(test_df))\n",
        "\n",
        "train_dataset = FashionDataset(train_df)\n",
        "test_dataset  = FashionDataset(test_df)\n",
        "\n",
        "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=True,collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False,collate_fn=collate_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuM15FmYEmV2"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(DEVICE)\n",
        "for p in model.vision_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "print(\"Using:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfVJYAWljTjX"
      },
      "outputs": [],
      "source": [
        "test = processor(text=[\"hello\"], return_tensors=\"pt\").to(DEVICE)\n",
        "out = model.get_text_features(**test)\n",
        "print(\"NaN in output:\", torch.isnan(out).any())\n",
        "print(\"Output sample:\", out[0][:5])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKsiE51AEo_0"
      },
      "outputs": [],
      "source": [
        "for p in model.vision_model.parameters():\n",
        "    p.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWIX4f18ErdM"
      },
      "outputs": [],
      "source": [
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=LR)\n",
        "num_steps = NUM_EPOCHS * len(train_loader)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    int(num_steps * WARMUP_RATIO),\n",
        "    num_steps\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnvRyoGuEu7o"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "MAX_GRAD_NORM = 1.0\n",
        "def train_epoch(epoch):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    start = time.time()\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} TRAIN\"):\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}   # <<< FIX\n",
        "        out = model(**batch, return_loss=True)\n",
        "        loss = out.loss\n",
        "        if torch.isnan(loss):\n",
        "            print(\" NaN detected — abort epoch\")\n",
        "            return\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        total_loss += loss.item()\n",
        "    avg = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} avg loss: {avg:.4f}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate():\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in tqdm(test_loader, desc=\"Test\"):\n",
        "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "        out = model(**batch, return_loss=True)\n",
        "        total_loss += out.loss.item()\n",
        "\n",
        "    avg = total_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {avg:.4f}\")\n",
        "    return avg\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXdNKPQgEyza"
      },
      "outputs": [],
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    train_epoch(epoch)\n",
        "    validate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19yS7l3-E5JG"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "@torch.no_grad()\n",
        "def compute_image_embeddings(model, processor, df):\n",
        "    model.eval()\n",
        "    all_embs = []\n",
        "    paths = df[\"image_path\"].tolist()\n",
        "    loader = DataLoader(paths, batch_size=32, shuffle=False)\n",
        "    for batch in loader:\n",
        "        imgs = [Image.open(p).convert(\"RGB\") for p in batch]\n",
        "        enc = processor(images=imgs, return_tensors=\"pt\").to(DEVICE)\n",
        "        # NO .half() here\n",
        "        feats = model.get_image_features(**enc)\n",
        "        # ensure FP32 and normalize\n",
        "        feats = feats.float()\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        all_embs.append(feats.cpu().numpy())\n",
        "    return np.vstack(all_embs)\n",
        "image_embs = compute_image_embeddings(model, processor, df)\n",
        "print(\"image_embs:\", image_embs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OfXEMifE8dV"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def compute_text_embeddings(model, processor, df):\n",
        "    model.eval()\n",
        "    all_embs = []\n",
        "    caps = df[\"caption\"].tolist()\n",
        "    loader = DataLoader(caps, batch_size=32, shuffle=False)\n",
        "    for batch in loader:\n",
        "        enc = processor(\n",
        "            text=batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(DEVICE)\n",
        "        feats = model.get_text_features(\n",
        "            input_ids=enc[\"input_ids\"],\n",
        "            attention_mask=enc[\"attention_mask\"])\n",
        "        feats = feats.float()\n",
        "        feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "        all_embs.append(feats.cpu().numpy())\n",
        "    return np.vstack(all_embs)\n",
        "text_embs = compute_text_embeddings(model, processor, df)\n",
        "print(\"text_embs:\", text_embs.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTj_EjbDNTcU"
      },
      "outputs": [],
      "source": [
        "!pip install -q faiss-cpu gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeWNkEZ2NWXl"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "image_embs_f32 = image_embs.astype(\"float32\")\n",
        "D = image_embs_f32.shape[1]\n",
        "index = faiss.IndexFlatIP(D)\n",
        "index.add(image_embs_f32)\n",
        "print(\"FAISS index size:\", index.ntotal)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwi6oxKANZXC"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from PIL import Image\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_text_query(query: str) -> np.ndarray:\n",
        "    model.eval()\n",
        "    enc = processor(\n",
        "        text=[query],\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    )\n",
        "    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n",
        "    feats = model.get_text_features(\n",
        "        input_ids=enc[\"input_ids\"],\n",
        "        attention_mask=enc[\"attention_mask\"])\n",
        "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    return feats.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "\n",
        "\n",
        "def search_text_to_image(query: str, top_k: int = 5):\n",
        "    q = encode_text_query(query)\n",
        "    scores, idxs = index.search(q, top_k * 10)  # fetch more\n",
        "\n",
        "    seen = set()\n",
        "    results = []\n",
        "\n",
        "    for score, idx in zip(scores[0], idxs[0]):\n",
        "        row = df.iloc[idx]\n",
        "        img_hash = row[\"phash\"]\n",
        "\n",
        "        if img_hash in seen:\n",
        "            continue\n",
        "\n",
        "        seen.add(img_hash)\n",
        "        results.append((row[\"image_path\"], row[\"caption\"], float(score)))\n",
        "\n",
        "        if len(results) == top_k:\n",
        "            break\n",
        "\n",
        "    return results\n",
        "\n",
        "# quick test\n",
        "results = search_text_to_image(\"men red sneakers\", top_k=5)\n",
        "#results_keyword= keyword_search(\"women red shoe\", top_k=5)\n",
        "for p, t, s in results:\n",
        "    print(s, \"->\", t, \"|\", p)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ODdZFS1Ndr_"
      },
      "outputs": [],
      "source": [
        "# Cosine similarity matrix (N x N)\n",
        "sims = text_embs @ image_embs.T   # both are normalized\n",
        "print(\"Similarity matrix:\", sims.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y58sbWPANiSm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "sims = text_embs @ image_embs.T   # (N x N)\n",
        "print(\"Similarity matrix computed:\", sims.shape)\n",
        "\n",
        "\n",
        "\n",
        "def compute_ranks(sim_matrix):\n",
        "    N = sim_matrix.shape[0]\n",
        "    ranks = []\n",
        "\n",
        "    for i in range(N):\n",
        "        order = np.argsort(-sim_matrix[i])   # descending\n",
        "        rank = int(np.where(order == i)[0][0])\n",
        "        ranks.append(rank)\n",
        "\n",
        "    return np.array(ranks)\n",
        "\n",
        "\n",
        "def recall_at_k(ranks, k):\n",
        "    return np.mean(ranks < k)\n",
        "\n",
        "\n",
        "def mean_average_precision(ranks):\n",
        "    # Only one relevant item → AP = 1/(rank+1)\n",
        "    ap = 1.0 / (ranks + 1)\n",
        "    return float(ap.mean())\n",
        "\n",
        "\n",
        "\n",
        "ranks = compute_ranks(sims)\n",
        "\n",
        "R1  = recall_at_k(ranks, 1)\n",
        "R5  = recall_at_k(ranks, 5)\n",
        "R10 = recall_at_k(ranks, 10)\n",
        "\n",
        "MedR = np.median(ranks)\n",
        "MnR  = np.mean(ranks)\n",
        "mAP  = mean_average_precision(ranks)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Total samples evaluated: {len(ranks)}\\n\")\n",
        "\n",
        "print(f\" Recall@1   : {R1:.4f}\")\n",
        "print(f\" Recall@5   : {R5:.4f}\")\n",
        "print(f\" Recall@10  : {R10:.4f}\\n\")\n",
        "print(f\" Median Rank: {MedR:.2f}\")\n",
        "print(f\" Mean Rank  : {MnR:.2f}\\n\")\n",
        "print(f\" mAP        : {mAP:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn0QCg1nNoT9"
      },
      "outputs": [],
      "source": [
        "def gradio_search(query: str, top_k: int = 5):\n",
        "    res = search_text_to_image(query, top_k=top_k)\n",
        "    images = [r[0] for r in res]  # image paths\n",
        "    captions = [f\"{r[1]} (score={r[2]:.3f})\" for r in res]\n",
        "    return images, \"\\n\\n\".join(captions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4582Je7NwpG"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def encode_single_image(path: str) -> np.ndarray:\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    enc = processor(images=[img], return_tensors=\"pt\").to(DEVICE)\n",
        "    if use_cuda:\n",
        "        enc[\"pixel_values\"] = enc[\"pixel_values\"].half()\n",
        "    feats = model.get_image_features(**enc)\n",
        "    feats = feats / feats.norm(dim=-1, keepdim=True)\n",
        "    return feats.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "\n",
        "def search_text_to_image(query: str, top_k):\n",
        "    q = encode_text_query(query)\n",
        "    scores, idxs = index.search(q, top_k * 3)  # fetch more\n",
        "\n",
        "    seen = set()\n",
        "    results = []\n",
        "\n",
        "    for score, idx in zip(scores[0], idxs[0]):\n",
        "        row = df.iloc[idx]\n",
        "        img_path = row[\"image_path\"]\n",
        "\n",
        "        if img_path in seen:\n",
        "            continue  # skip duplicates\n",
        "\n",
        "        seen.add(img_path)\n",
        "        results.append((img_path, row[\"caption\"], float(score)))\n",
        "\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeqHuuOFNrim"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"##  Fashion CLIP Search \")\n",
        "\n",
        "    with gr.Row():\n",
        "        query = gr.Textbox(label=\"Text query\", value=\"boys tee\", lines=1)\n",
        "        topk  = gr.Slider(label=\"Top K\",value=5, step=1)\n",
        "\n",
        "    search_btn = gr.Button(\"Search\")\n",
        "\n",
        "    # Fixed gallery (no .style())\n",
        "    gallery = gr.Gallery(\n",
        "        label=\"Results\",\n",
        "        columns=5,\n",
        "        height=\"auto\"\n",
        "    )\n",
        "\n",
        "    captions_box = gr.Textbox(\n",
        "        label=\"Captions & Scores\",\n",
        "        lines=10\n",
        "    )\n",
        "\n",
        "    search_btn.click(\n",
        "        fn=gradio_search,\n",
        "        inputs=[query, topk],\n",
        "        outputs=[gallery, captions_box]\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVTPb19tNvgl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPLM0ZyrO52OC/QbP7zQjcc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}